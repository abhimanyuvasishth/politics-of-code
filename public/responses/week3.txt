It's easy to forget that machine learning algorithms, even the most sophisticated, were coded by humans and trained on data provided by humans on criteria set by humans. Thus, human biases are often transferred to these algorithms and this is glaringly apparent in 'Machine Bias' and 'Man is to Computer Programmer as Woman is to Homemaker?' The authors of 'Machine Bias' called their article an investigation into the algorithms that control our lives. And for a black person who is unfairly flagged as a potential criminal, algorithms are important. Understanding the subjective nuances of training data used in machine learning algorithms is crucial, and 'Man is...' explores this concept. A NLP algorithm that ran on data we find online would be exposed to the biases and stereotypes of word associations on the internet. We've seen Microsoft's AI chatbot that came up with racist tweets after 'learning' from numerous tweets on Twitter. Or Google's photo recognition algorithm that mistakenly tagged a few black people as gorillas. Or Nikon's smart camera that thought East Asian people were blinking. But a machine need not be fraught with the same biases that many internet users have, and exercises like the ones carried out by the authors of 'Man is...' are necessary to put training data into context, especially since these biases are not simply transferred but are amplified in learning algorithms, making lives worse for the already marginalized. Software embodies values. And especially now, with AI/robots/learning algorithms constantly peeping into our lives looking for data, it is imperative to realize that these systems are not neutral pieces of code The point Nissenbaum is trying to make is that these values are a big deal. And they are. We need to know if a certain voice-controlled TV monitor is continuously listening to you, or if a company is willing to sell your private data for dollars in ad revenue before making a purchase. 